# -*- coding: utf-8 -*-
"""Gym_ChatBot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16S5UrCN7cMfBOwuWkSaTyYjcRQJSzHgm
"""

!pip install wikipedia

import pandas as pd
import nltk
nltk.download('stopwords')
nltk.download('punkt')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
import wikipedia
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from google.colab import files
upload=files.upload()

data=pd.read_csv("megaGymDataset.csv")
print(data.head())

print(data.columns)

#Counting occurrences of a categorical variable :- Strength
category_counts = data["Type"].value_counts()
category_counts.plot(kind="bar")
plt.xlabel("Category")
plt.ylabel("Count")
plt.title("Distribution of Categories in the Dataset")
plt.show()

category_counts = data["BodyPart"].value_counts()
category_counts.plot(kind="bar")
plt.xlabel("Category")
plt.ylabel("Count")
plt.title("Distribution of Categories in the Dataset")
plt.show()

category_counts = data["Equipment"].value_counts()
category_counts.plot(kind="bar")
plt.xlabel("Category")
plt.ylabel("Count")
plt.title("Distribution of Categories in the Dataset")
plt.show()

# Preprocess the data
stop_words = set(stopwords.words('english'))
ps = PorterStemmer()

def preprocess_text(text):
    if pd.isna(text) or not isinstance(text, str):
        return ''  # Return an empty string for missing/NaN values
    tokens = word_tokenize(text.lower())
    filtered_tokens = [ps.stem(token) for token in tokens if token not in stop_words]
    return ' '.join(filtered_tokens)

data['Desc'] = data['Desc'].apply(preprocess_text)

# Create a dictionary for mapping descriptions to exercises
exercise_dict = dict(zip(data['Desc'], data['Title']))

# Define a function to find the closest exercise
def find_closest_exercise(user_input):
    vectorizer = TfidfVectorizer()
    input_vector = vectorizer.fit_transform([preprocess_text(user_input)])
    exercise_vectors = vectorizer.transform(data['Desc'])
    similarities = cosine_similarity(input_vector, exercise_vectors)
    closest_index = similarities.argmax()
    closest_exercise = data.iloc[closest_index]['Title']
    return closest_exercise

# Chat bot loop
print("Bot: Hi! I'm a gym chat bot. How can I assist you today?")
while True:
    user_input = input("You: ")
    if user_input.lower() == "bye":
        print("Bot: Goodbye! Have a great day.")
        break

    closest_exercise = find_closest_exercise(user_input)
    if closest_exercise:
        exercise_details = data.loc[data['Title'] == closest_exercise].iloc[0]
        print(f"Bot: The exercise you might be looking for is: {closest_exercise}")
        print(f"Description: {exercise_details['Desc']}")
        print(f"Type: {exercise_details['Type']}")
        print(f"Body Part: {exercise_details['BodyPart']}")
        print(f"Equipment: {exercise_details['Equipment']}")
        print(f"Level: {exercise_details['Level']}")
        print(f"Rating: {exercise_details['Rating']} ({exercise_details['RatingDesc']})")
    else:
        try:
            # Fetch information from Wikipedia
            wiki_page = wikipedia.page(user_input)
            summary = wiki_page.content[:500].strip('=\n ')
            print(f"Bot: {summary}...")
        except wikipedia.exceptions.DisambiguationError as e:
            print(f"Bot: There are multiple pages related to '{user_input}'. Please be more specific.")
        except wikipedia.exceptions.PageError:
            print("Bot: I'm sorry, I couldn't find any information related to your query.")
        except Exception as e:
            print(f"Bot: An error occurred while processing your query: {e}")